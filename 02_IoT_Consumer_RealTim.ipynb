{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4d8f5be-0eb6-4656-8bc0-07f9fb50c767",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json, col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "eh_namespace = \"eh-retail-stream-tithir\"\n",
    "eh_name = \"telemetry_stream\"\n",
    "#Connection String\n",
    "conn_string = \"Endpoint=sb://eh-retail-stream-tithir.servicebus.windows.net/;SharedAccessKeyName=RootManageSharedAccessKey;SharedAccessKey=zh4CpF2GknXjtj7o/JRpqJE17mPE/RGTV+AEhJzqptE=\"\n",
    "\n",
    "# JAAS Config for Spark-Kafka Connector\n",
    "jaas_config = f'org.apache.kafka.common.security.plain.PlainLoginModule required username=\"$ConnectionString\" password=\"{conn_string}\";'\n",
    "\n",
    "kafka_options = {\n",
    "    \"kafka.bootstrap.servers\": f\"{eh_namespace}.servicebus.windows.net:9093\",\n",
    "    \"kafka.sasl.mechanism\": \"PLAIN\",\n",
    "    \"kafka.security.protocol\": \"SASL_SSL\",\n",
    "    \"kafka.request.timeout.ms\": \"60000\",\n",
    "    \"kafka.sasl.jaas.config\": jaas_config,\n",
    "    \"subscribe\": eh_name,\n",
    "    \"startingOffsets\": \"latest\"\n",
    "}\n",
    "\n",
    "# --- READ STREAM ---\n",
    "df_raw = spark.readStream.format(\"kafka\").options(**kafka_options).load()\n",
    "\n",
    "# Define Schema\n",
    "schema = StructType([\n",
    "    StructField(\"device_id\", StringType()),\n",
    "    StructField(\"temperature\", DoubleType()),\n",
    "    StructField(\"humidity\", DoubleType()),\n",
    "    StructField(\"status\", StringType()),\n",
    "    StructField(\"timestamp\", DoubleType())\n",
    "])\n",
    "\n",
    "# Parse JSON\n",
    "df_parsed = (df_raw\n",
    "    .select(col(\"value\").cast(\"string\").alias(\"json_payload\"))\n",
    "    .select(from_json(col(\"json_payload\"), schema).alias(\"data\"))\n",
    "    .select(\"data.*\"))\n",
    "\n",
    "# --- WRITE TO DELTA TABLE ---\n",
    "checkpoint_path = \"/Volumes/azure_databricks_personal/default/volume_checkpoints/iot_spark_streaming_v2\"\n",
    "\n",
    "(df_parsed.writeStream\n",
    "    .format(\"delta\")\n",
    "    .option(\"checkpointLocation\", checkpoint_path)\n",
    "    .outputMode(\"append\")\n",
    "    .toTable(\"azure_databricks_personal.default.iot_telemetry_realtime\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3434f5f-779a-46da-9e2e-89820ef587bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_IoT_Consumer_RealTim",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
